# <system>
#   workers 4
# </system>

<source>
  @type forward
  port 24224
</source>

<match fluent.**>
  @type null
</match>

# Gen hash id - https://medium.com/redbox-techblog/tuning-fluentd-retries-avoiding-duplicate-documents-in-elasticsearch-e7cb9630a453
<filter **>
  @type elasticsearch_genid
  hash_id_key _hash
</filter>

<filter docker>
  @type concat
  key log
  stream_identity_key container_id
  multiline_start_regexp /^\d{4}-\d{2}-\d{2}/
  flush_interval 5
  timeout_label "@NORMAL"
</filter>

<match **>
  @type relabel
  @label @NORMAL
</match>

<label @NORMAL>
  <filter docker>
    @type parser
    format json
    key_name log
    reserve_data true
    reserve_time true
    time_format "%Y-%m-%dT%H:%M:%S.%NZ"

    <parse>
      @type grok
      grok_name_key grok_name
      grok_failure_key grokfailure

      <grok>
        name basic
        pattern ^\[%{DATA:class}\s*\|\s*%{GREEDYDATA:log_time}\]\s*%{GREEDYDATA:message}
      </grok>
      <grok>
        name java_logs-short
        pattern ^%{DATA:log_time}\s+%{LOGLEVEL:log_level}\s+%{NUMBER:pid}\s+---\s+\[\s*%{DATA:thread}\]\s+%{DATA:class}\s+:\s+%{GREEDYDATA:message}
      </grok>
      <grok>
        name java_logs-detail
        pattern ^%{DATA:log_time}\s+%{LOGLEVEL:log_level}\s+\[%{DATA:component_name},%{DATA:trace_id},%{DATA:span_id},%{DATA:exported_to_zipkins}\]\s+%{NUMBER:pid}\s+---\s+\[\s*%{DATA:thread}\]\s+%{DATA:class}\s+:\s+([ :]*%{DATA:structure_format} -- %{DATA:structure_index} -- )?([ :]*%{DATA:ordering_id} :: %{DATA:execution_id} :: )?%{GREEDYDATA:message}
      </grok>
    </parse>
  </filter>

  <match docker>
    @type copy
    <store>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"

      log_es_400_reason true

      id_key _hash # specify same key name which is specified in hash_id_key
      remove_keys _hash # Elasticsearch doesn't like keys that start with _
      include_tag_key true
      include_timestamp true

      logstash_format true
      logstash_prefix fluent
      logstash_dateformat %Y.%m.%d

      # time_key time
      time_key_format %Y-%m-%dT%H:%M:%S.%NZ
      time_precision 9

      flush_interval 5s

      <buffer>
        @type file
        path /home/fluent/buffer

        total_limit_size 8192MB
        # chuck + enqueue
        chunk_limit_size 16MB
        flush_mode interval
        flush_interval 20s
        # flush thread
        flush_thread_count 128

        flush_at_shutdown true

        retry_wait 1                      # The wait interval for the first retry.
        retry_exponential_backoff_base 2  # Increase the wait time by a factor of N.
        retry_type exponential_backoff    # Set 'periodic' for constant intervals.
        retry_max_interval 1h             # Cap the wait interval. (see above)
        retry_randomize true              # Apply randomization. (see above)
        retry_timeout 8h                  # Maximum duration before giving up.
        retry_forever false               # Set 'true' for infinite retry loops.
        retry_secondary_threshold 0.8     # See the "Secondary Output" section in

        overflow_action drop_oldest_chunk
      </buffer>
    </store>
    <store>
      @type file
      path /fluentd/output/buffer2/
    </store>    
  </match>

</label>